[{"authors":["admin"],"categories":null,"content":"Kenneth completed his bachelor\u0026rsquo;s degree in Mathematics and Computer Science at Brigham Young University-Hawaii. Currently, he is pursuing a M.S. in Statistics at UC Davis. His research interests lie in causal inference, robotics, grahpical models, optimal control, and machine learning, especially on reinforcement learning.\nIn his spare time, he likes reading books, cooking, hiking, and traveling. Above all, he loves learning new things. His recent favorite book is Causality: Models, Reasoning and Inference by Judea Pearl.\n","date":1530403200,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1530403200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://kenneth-lee-ch.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Kenneth completed his bachelor\u0026rsquo;s degree in Mathematics and Computer Science at Brigham Young University-Hawaii. Currently, he is pursuing a M.S. in Statistics at UC Davis. His research interests lie in causal inference, robotics, grahpical models, optimal control, and machine learning, especially on reinforcement learning.\nIn his spare time, he likes reading books, cooking, hiking, and traveling. Above all, he loves learning new things. His recent favorite book is Causality: Models, Reasoning and Inference by Judea Pearl.","tags":null,"title":"Kenneth Lee","type":"authors"},{"authors":null,"categories":null,"content":"This work has shown a way to estimate the effect of the emergency declaration on mobility during the pandemic. The emegency declaration tend to be more effective in reducing mobility in the areas that have large population, small percent of people in poverty, high percent of people with education backgrounds, low unemployment rate. One can apply the same strategy to estimate the causal effect of a single intervention so long there is no any other interventions happening concurrently.\nAlso, we provide a way to estimate the effects of interventions when the potential confounding variables are observed. Having accounted for case count signals and number of outpatient visits, we see that governemnt interventions can be more significiant in reducing the mobility in terms of restaurant visit. For example, in Allegaheny county in Pennsylvania, among all governemnt interventions, only mandatory stay at home order reduces restaurant visit significantly at 0.05 significant level. On the other hand, bar restriction and gathering restriction significantly reduce restaurant visit in Yolo county in California in comparison with other interventions.\nWe leave characterization for the ranks of the effect of interventions on county-level as a future work. Other regression methods such as non-parametric regression such as generalized additive models can also be used for further study. One should note that the effects of the interventions vary across counties in general. This study assumes that every county strictly follows all state-wide policies. It is encouraged to study a specific county in order to make a more precise conclusion on the effects of the intervention.\n","date":1607731200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607731200,"objectID":"6d683cce0a388c80d1ff0c6436910e8d","permalink":"https://kenneth-lee-ch.github.io/project/covid19/","publishdate":"2020-12-12T00:00:00Z","relpermalink":"/project/covid19/","section":"project","summary":"Mobility is one of the potential factors in contributing to the spread of COVID-19. Our goal is to estimate the effect of state-wide policies on mobility signals at a county-level.","tags":["Machine Learning","Statistics","Healthcare"],"title":"Impact of Interventions seen through mobility","type":"project"},{"authors":null,"categories":null,"content":" Introduction Understanding what descriptions lead to certain sentiment in language is important nowadays when reactions to various decisions are made in seconds on social media. The goal of this project is to construct a model for a given sentence and the label sentiment to predict what phrases in the sentence that best support the given sentiment. In natural language processing, we can formulate this task as a question-answering task where sentiment is a question, the tweet is the context, and the support phrase is the answer. We compare several popular neural network models such as the Long-short Term Memory (LSTM), Gated Recurrent Unit (GRU) for this task. Under a word similarity metric, Jaccard score, we are able to achieve 0.607 score with the model.\nMethods and Results Exploratory Data Analysis and Processing Prior to model building, it is always good to explore the dataset first. From the distribution of the sentiment in the tweets, we can see that tweets that carry netrual sentiment tend to be shorter than those that are positive or negative. We also see that negative sentiment frequently occurs at text length around 10 or 20. The collected tweets hardly go over 35 words.\nNext, we can get a sense of what words that contribute to different sentiments in the tweets. We plot three different wordclouds to show different collections of vocabularies that count towards different types of sentiment. For the positive wordcloud, the main words include \u0026ldquo;love\u0026rdquo;, \u0026ldquo;thank\u0026rdquo;, \u0026ldquo;good\u0026rdquo;.\nInterestingly, as we look at the neutral word cloud, we see that the main words include \u0026ldquo;today\u0026rdquo;, \u0026ldquo;going\u0026rdquo;, \u0026ldquo;want\u0026rdquo;, \u0026ldquo;need\u0026rdquo;.\nThen, we can see from the negative word cloud that people mostly express their negative sentiment through words like \u0026ldquo;really\u0026rdquo;, \u0026ldquo;sad\u0026rdquo;, \u0026ldquo;sorry\u0026rdquo;.\nBesides, we can look at two words together at a time, which is known as bigram, to understand which two words go together often in different sentimental tweets. We can see that \u0026ldquo;mother day\u0026rdquo; and \u0026ldquo;happy mother\u0026rdquo; are strongly associated with positive sentiment. In addition, negative sentiment is mostly associated with \u0026ldquo;feel like\u0026rdquo; and \u0026ldquo;last night\u0026rdquo;. It maybe good to look into those tweets with \u0026ldquo;last night\u0026rdquo; to see if most of them come from news.\nEvaluation Metric The metric in this project is the word-level Jaccard score as follows:\nwhere A, B are sets of words, and |.| denotes the cardinality.\nBidirectional LSTM and GRU with word embedding The first two neural network architectures we use learn to predict a binary sequence to find the positions corresponding to the selected text inside a given sentence. It consists of the following components: a bidirectional LSTM layer (interchangeable with a GRU layer to become a bidirectional GRU (B-GRU)), a dropout layer, a fully connected layer, a dropout layer, a fully connected layer, a dropout layer, and an output layer. Time distributed layer is added in between droput, full-connected layer, and the output layer.\nWe use the Keras API in Tensorflow to build a model that consist of the following components in order:\n Input layer with maximum list length among all the sentences in text column in the data. It can be incorporated in the bidirectional layer argument.\n A bidirectional LSTM/GRU layer with 20 units, set return sequence = True, dropout=0.3, recurrent dropout = 0.3.\n A dropout layer with 0.5 rate followed by a fully connected layer with 64 units and set kernel constraint=max norm(3). When use LSTM layer, change the activation in the first layer to be “relu” and the second activation to be “tanh”. When using GRU, the first activation should be “tanh” and the second activation to be “relu”.\n Repeat the same set up in 4.\n A dropout layer with 0.5 rate followed by a one unit output layer with sigmoid activation, which is for binary classification.\n  Lastly, set the loss function to be binary cross entropy and use SGD(lr=0.1 , momentum=0.9) as the optimizer with accuracy as the metric for compiling model. Then, fit the model with training data and validate on the test set with 32 batch size, 60 epochs. At the end, find all the text from the test set that corresponds 1’s in the predicted vector by using tokenizer to output the corresponding text and compute the Jaccard score.\nImplementation Summary  Replace any word in the text with \u0026lt;token\u0026gt; if it matches the selected text and create a new column in the dataframe called tokenized text.\n Use Tokenizer() from tensorflow package to tokenize all sentences in text and tokenized text columns. Specify the parameter Tokenizer(Filter=“”) so it will recognize \u0026lt;token\u0026gt; as a word.\n Ensure all the words that contain \u0026lt;token\u0026gt; in the tokenizer have the same index in the tokenizer. Then, get the length of the list that contains the longest text in the text column. We then convert each row in text to a list of integers by the indicies in tokenizer and pad all lists to have the same length by the max length we obtained previously with zero, call it X. We also convert the text in tokenized text in the same manner, call it y.\n For every word in each text in y, if it doesn’t correspond to the index of \u0026lt;token\u0026gt;, we set it to be 0 and 1 otherwise.\n Then, we randomly shuffle and split X and y with 20% test set for testing the model.\n Use np.array(X_train).reshape(X_train.shape[0], X_train.shape[1],1).astype(np.float32) to transform the dimension for the training samples and np.array(y_train).reshape(y_train.shape[0], y_train.shape[1],1) to transform the dimension for the response in order to have the model to output a sequence.\n  Justification for Parameters Use As recommended by Srivastava et al., 2014, we set a large learning rate and high momentum for the SGD along with dropout and follow a similar fashion to set the kernel_constraint on the weights for each hidden layer, ensuring that the maximum norm of the weights does not exceed a value of 2 . Also, we set 60 epochs as we see the training and validation loss don’t change much after 60 epochs. The increases in learning rate is also recommended by (Srivastava et al., 2014) while using dropout. We set the recurrent dropout in the LSTM/GRU layer as suggested by Gal \u0026amp; Ghahramani, 2016 to use with regular dropout. Setting 0.5 rate is via a process of trial and error.\nWe tuned various activation functions (tanh, ReLU, sigmoid) for two hidden layers with 20 units and initialized LSTM/GRU with 20 units. Then, after selected a desirable combination of activation functions, we tuned the number of nodes for the LSTM, GRU, and those two hideen layers with 30 and 64 units. We also compare the model performance with and without GloVe embedding for both GRU and LSTM models. Also, we have tried using the default Adam optimizer in keras, but the model has suffered from overfitting. However, we don’t see much improvement by increasing the number of nodes, we decide to use 20 number of unit for computational advantages.\nResult and Conclusion Both the B-LSTM and B-GRU achieve 0.607 Jaccard score. GRU and LSTM perform the same on this task based on our tuned parameters. The process of tuning parameters for neural network model is extremly time-consuming especially when the data dimension is large. For the LSTM and GRU model, we first attempted to try different optimizers, then different number of units for hidden layers, activation functions and number of layers. There are a lot of modification to our codes since we attempt to build a better model. We have also tried using pretained embedding layer such as GloVe, but it was too expensive to train, given the dimension of the embedding and we may have to retune all the parameters. In addition, we use a tokenizer to convert text signal for the LSTM/GRU architecture, however, it may lose the information of the context of the sentence and maybe that’s one of the reasons why BERT seems to perform better than LSTM or GRU for this task.\nData Source Tweet Sentiment Extraction\nReference  Kaggle:tweet sentiment extraction. https://www.kaggle.com/c/ tweet-sentiment-extraction.\n Cho, K., Van Merrienboer, B., Bahdanau, D., and Bengio, Y.¨ On the properties of neural machine translation: Encoderdecoder approaches. arXiv preprint arXiv:1409.1259, 2014.\n Chung, J., Gulcehre, C., Cho, K., and Bengio, Y. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.\n Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805v2, 2018.\n Gal, Y. and Ghahramani, Z. A theoretically grounded application of dropout in recurrent neural networks. In Advances in neural information processing systems, pp. 1019–1027, 2016.\n Hochreiter, S. and Schmidhuber, J. Long short-term memory. Neural computation, 9(8):1735–1780, 1997.\n Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research, 15(1):1929–1958, 2014.\n  ","date":1591920000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591920000,"objectID":"38ada3812c16ae0cd4dc2e212c6c7821","permalink":"https://kenneth-lee-ch.github.io/project/supportphraseprediction/","publishdate":"2020-06-12T00:00:00Z","relpermalink":"/project/supportphraseprediction/","section":"project","summary":"The goal of this project is to construct a model for a given sentence and the label sentiment to predict what phrases in the sentence that best support the given sentiment.","tags":["Machine Learning","Statistics","Neural Networks","Classification"],"title":"Tweet Sentiment Extraction","type":"project"},{"authors":null,"categories":null,"content":"We explore the potential of creating new recipes via text data. Our goal has two folds. First, we aim to classify the cuisine based on ingredients. Second, we want to predict an ingredient that is missing from a given list of ingredients and a cuisine name. The first task can be formulated as a multi-class classification problem. To convert the text into numerical signals, we can use TFIDF vectorizer, Countvectorizer, word embedding based on Word2Vec model. We are able to achieve 0.85 micro-averaged F1 scores for the multi-classification task with multilayer perceptrons and bag-of-words model.\nFor the first task, we compare several well-known classification algorithms such as logistic regression, naive bayes, linear discriminant analysis, decision tree classifier, random forest, Adaboost, multi-layer perceptrons. We conduct grid search with 5-fold stratified cross-validation for hyperparameter tuning.\nFor the second task, we adopt two approaches to process the recipe text, which is the key to the recommender system. We first explore the recommended ingredients based on similarity to the given recipe using vectorizers; and then we examine the performance, in terms of the \u0026ldquo;top n accuracy\u0026rdquo; metric, of a baseline popularity model and a sophiticated collaborative filtering model under the ``text_preprocess method.\n","date":1591920000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591920000,"objectID":"c6d0ab8248c0c068244cae04bfcaab3f","permalink":"https://kenneth-lee-ch.github.io/project/what2cook/","publishdate":"2020-06-12T00:00:00Z","relpermalink":"/project/what2cook/","section":"project","summary":"We explore the potential of creating new recipes via text data. Our goal has two folds. First, we aim to classify the cuisine based on ingredients. Second, we want to predict an ingredient that is missing from a given list of ingredients and a cuisine name.","tags":["Machine Learning","Recommender Systems","Classification","Clustering","Neural Networks"],"title":"What2Cook","type":"project"},{"authors":null,"categories":null,"content":"Citibike is New York City’s bike share system established in 2013. It has become a vital part of the transportation network in New York City. It is available for use throughout the entire year. All the riders have access to thousands of bikes at hundreds of stations across Manhattan, Brooklyn, Queens and Jersey City. In this work, we would like to explore the characteristics of this bike share system and come up with recommendations that may improve the current program based on data analysis wtih the Citi Bike system 2019 data that is publicly available.\nAccording to a New York magazine, Intelligencer (https://nymag.com/intelligencer/2014/03/citi-bikes-four-huge-problems.html), there are some problems the citi-bike share system faced in 2014. We summarize them as follows:\n How can we increase the number of customer (one-time user)?\n Do we have any suggestion to attract more users, especially customer, during Winter?\n Riders complained as bikes went missing for maintenance or protection, leaving certain stations empty for days at a time, what can we do with that problem?\n  In this work, we would like to explore the characteristics of this bike share system and try to come up with recommendations to provide potential solutions for the questions above based on data analysis wtih the Citi Bike system 2019 data that is publicly available.\n","date":1583798400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583798400,"objectID":"f7397d1e0158418f05d7871310a24d73","permalink":"https://kenneth-lee-ch.github.io/project/citibike/","publishdate":"2020-03-10T00:00:00Z","relpermalink":"/project/citibike/","section":"project","summary":"In this work, we would like to explore the characteristics of this bike share system and try to come up with recommendations to provide potential solutions for the questions above based on data analysis wtih the Citi Bike system 2019 data that is publicly available.","tags":["Machine Learning","Visualization","Clustering","Transportation"],"title":"Citibike System Data Analysis","type":"project"},{"authors":null,"categories":null,"content":"The effects of class sizes on student achievement is an important topic for policymakers in the American K-12 education system. To study the effects of class size on student achievement in the primary grades, the State Department of Education in Tennessee launched a four-year longitudinal class-size randomized study from 1985 to 1989 called The Student/Teacher Achievement Ratio (STAR). Over 7000 students in 79 schools participated in this project. We highlight the features of the experiment process in the study below.\n All participating schools had to agree to the random assignment of teachers and students to different class conditions: small class (13 to 17 students per teacher), regular class (22 to 25 students per teacher), and regular-with-aide class (22 to 25 students with a full-time teacher’s aide).\n The assignments of various class types were initiated as the students entered school in kindergarten and continued through third grade.\n Each school must provide enough kindergarten students to be assigned to three numerous class types in order to participate in the project STAR.\n The student achievement is measured annually via Stanford Achievement Tests (SATs) during the spring term on testing dates specified by the Tennessee state.\n Students moving from a school involved in STAR to another participating school were assigned to the same type of class as they had participated in previously. Also, it is possible that the size of a regular class can be as small as the small class type as students move out of the participating schools.\n Besides class size and teacher aides, there were no other experimental changes involved in the study.\n There were three schools resigned from the project STAR at the end of kindergarten, so that there were only left with 76 schools in the 1st-grade level.\n  Our primary scientific question of interest is whether there is a treatment effect of assigning various class types to the average math scaled scores in a 1st-grade class level. We implement exploratory data analysis, two-way ANOVA model, model diagnostics, hypothesis testing. In the end, we will discuss any causal statements that could possibly be made based on our analysis and assumptions and the differences between a student-level and a class-level analysis on this STAR dataset.\nThis work shows that the treatment effect of the class type does exist in a class level for this dataset. We also show that it is possible to make causal statements based on our analysis.\n","date":1583798400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583798400,"objectID":"f6143dffba2f036cf73abd4a22782e84","permalink":"https://kenneth-lee-ch.github.io/project/star/","publishdate":"2020-03-10T00:00:00Z","relpermalink":"/project/star/","section":"project","summary":"This report is an analysis of STAR (Student-Teacher Achievement Ratio) project data specifically targeted the 1st graders’ dataset with teacher as the unit (class-level average score).","tags":["Statistics","Block Design","Randomized Experiment","ANOVA","Casual Inference"],"title":"Student/Teacher Achievement Ratio","type":"project"},{"authors":null,"categories":null,"content":"Credit card fraud detection is one of the most important issues for credit card companies to deal with in order to earn trust from its customers. As machine learning techniques are robust to many tackle classification problems settings such as image recognition, we aim to explore various machine learning classification algorithms on this particular problem of classifying credit card fraud.\nThis work showcases on how to compare different algorithms and fine-tune them. The dataset mainly contains 492 frauds out of 284,807 transactions. It has 28 principle components, transaction time, and tranaction amount with labels, 0 being non-fraud and 1 being fraud.\n","date":1578614400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578614400,"objectID":"b737c71776be010e5d00d6af4738beb7","permalink":"https://kenneth-lee-ch.github.io/project/creditfraud/","publishdate":"2020-01-10T00:00:00Z","relpermalink":"/project/creditfraud/","section":"project","summary":"It classifies fraudulent credit card transactions based on the result of a PCA transformation. We explore various classic classification algorithms in this work.","tags":["Machine Learning","Classification","Credit Fraud Detection","Neural Networks"],"title":"Credit Card Fraud Detection","type":"project"},{"authors":null,"categories":null,"content":"In this study, we aim to resolve the traditionally tedious and time consuming task of determining the age of abalone by constructing predictive models of the ages of abalones using other physical measures that are easier to obtain. Based on the dataset with which we are provided, we construct models of various complexities and propose the one with the highest out-of-sample prediction performance. Our model is able to control the mean squared prediction error to an infinitesimal extent, while subject to only a reasonably small model bias. In addition, we also explore the possibilities of conjecturing other interior content (in-shell information), such as shucked weights of abalone, using measurements which can be obtained prior to cracking the shell (out-of-shell information). Our results show that not only is predicting such in-shell measures with out-of-shell information feasible, but one might achieve even better results than making predictions on the ages of the abalones.\n","date":1570665600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570665600,"objectID":"8d9d0ce1e08a893a2210d1db2556fdd8","permalink":"https://kenneth-lee-ch.github.io/project/abalone/","publishdate":"2019-10-10T00:00:00Z","relpermalink":"/project/abalone/","section":"project","summary":"We aim to resolve the traditionally tedious and time consuming task of determining the age of abalone by constructing predictive models of the ages of abalones using other physical measures that are easier to obtain.","tags":["Machine Learning","Regression","Marine Biology"],"title":"Abalone's Age Prediction","type":"project"},{"authors":null,"categories":null,"content":" In this project, we aim to train a classifier to accurately classify a person whether or not has a heart disease problem. In particular, we would like to compare several classification algorithms: logistic regression, random forests, support vector machine and k-nearest neighbours based on sensitivity and classification accuracy. Also, it will be beneficial to see if we can reduce the number of features to perform a good classification that may potentially lower the costs of collecting expensive features in the future. Most people believe that age should be a strong indicator whether a person has a heart disease problem. Our project will also validate this claim and see if there are any better indicators with hypothesis testing.\nQuestions of interests What is the relationship between age and heart disease?\nWill combining several predictors into one new predictor variable be useful to improve our model?\nWhat will be the best combination of features we should use based on the classification algorithm?\nCan we reduce the number of features and still get a satisfactory classification performance?\nWhat is the best classification rate that we can come up with to predict the presence of heart disease?\nData Exploration We first explore the data by making use of boxplots, histograms, density plots, and a scatterplot matrix. In particular, we use boxplots to explore the distribution of some quantitative variables such as maximum heart rate achieved, serum cholesterol, age, resting blood pressure and by gender and disease to see if there is any significant differences among different genders and types of disease.\nThen, we would split the data set into training and testing datasets by having 80 percent of the data to be training and 20 percent of the data to be testing. We then implemented several algorithms including logistic regression, random forests, support vector machine, and K-nearest neighbors to classify the presence and absence of heart disease. We also conduct outlier detection to see if there is a performance gain by dropping some potential outliers in the data.\nClassification Model Logistic Regression We first compare two logistic models that use all the predictor variables: model 1 and model 2. The difference between these two models is that we dropped two potential outliers (row 259 and 4) by looking at the standardized deviance residual plot and use the rest of the data to train the model 2 to see if there is a performance gain in comparison with model 1.\nNext, we also train three logistic regression models with different numbers of predictors. Model 3 utilizes all first order term and two-way interaction term. Model 4 is only trained with quantitative predictor variables including age, resting blood pressure, serum cholesterol, maximum heart rate achieved, oldpeak, and major vessels. Model 5 will be trained by a reduced number of features based on our feature selection method. Then, we will select the model with the highest accuracy and sensitivity rate to compare with other algorithms.\nProbability threshold selction for logistic regression classification Besides, we use Receiver Operating Characteristics (ROC) curve to see what are some best cut-off threshold for getting the most accurate classification rate for each model by picking thresholds that maximizes the area under the curve as it measures how good our model is able to distinguish two classes of the response variable.\nFeatures selection We want to estimate the variable importance to see if we can reduce the number of features and still achieve a satisfactory result. The importance can be estimated using a ROC curve analysis conducted for each attribute.\nEvaluation Metric Our evaluation metric is based on sensitivity and the accuracy rate. Since it is more important to correctly identify people who have a heart disease, we set the presence of heart disease as positive. Sensitivity is the true positive rate, which is the percentage of people with heart disease who are correctly identified as having the condition.\nConclusion In conclusion, we see people who have heart disease are mostly around the age of 60. Also, the predictor variable, age, doesn’t explain the response variable of heart disease as good as the number of major vessels in our logistic regression model. Moreover, we see that combining features into one predictor variable by using two-way interaction terms does not necessarily improve classification performance in general. When we look at our logistic regression models, we see that reducing number of features for training can improve model performance by excluding the predictor named fasting blood sugar. We have found that the following predictors are particularly important in logistic regression model for classification: major vessels, chest pain type, thal, sex, resting blood pressure, slope, oldpeak, serum cholestoral, exercise induced angina and maximum heart rate achieved.\nData Source UCI heart disease data set\n","date":1570665600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570665600,"objectID":"6ca6474d2b5c22eafdf704aeb70e5fb7","permalink":"https://kenneth-lee-ch.github.io/project/heart/","publishdate":"2019-10-10T00:00:00Z","relpermalink":"/project/heart/","section":"project","summary":"We aim to train a classifier to accurately classify a person whether or not has a heart disease problem. The dataset is provided by UCI Machine Learning Repository.","tags":["Machine Learning","Classification","Statistics","Healthcare"],"title":"Heart Disease Classification","type":"project"},{"authors":null,"categories":null,"content":"Spam, a bulk of unsolicited messages sent by anonymous sources, has been a costly issue to human communication. Machine learning techniques have been shown promising to filter these messages as it adapts to the evolving characteristics of the spam. In this work, we focus on neural networks to the problem of spam filtering. Overall, we conclude that using bidirectioncomparing various classification methodsal gated recurrent neural network with tokenizer method is the most robust way we have found to handle this problem with our particular dataset. This work provides insights on how to design a neural network to work with spam filtering problem as part of my contribution to a course project titled \u0026ldquo;Machine_Learning_Approaches_to_Spam_Filtering_Problems\u0026rdquo;. You may click on the PDF file to view the full paper.\n","date":1568073600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568073600,"objectID":"a8c6e6cba869c589c9eaa0ddc0d4e441","permalink":"https://kenneth-lee-ch.github.io/project/spam/","publishdate":"2019-09-10T00:00:00Z","relpermalink":"/project/spam/","section":"project","summary":"This work provides insights on how to design a neural network to work with spam filtering problem.","tags":["Machine Learning","Classification","Neural Networks"],"title":"SMS Spam Classification","type":"project"},{"authors":null,"categories":null,"content":"We estimated probabilities for tetrahedral and triangular dice using the sphere projection method and multivariate calculus. To test our results, we printed several dice of varying sizes. After a few thousand initial rolls, we realized that our calculations weren’t accurately describing the situation and that bias could have been introduced due to rolling the dice by hand. Therefore, we built a machine to roll the dice.\n","date":1533859200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1533859200,"objectID":"0819ecf4a21961a2dd14fd5b9981b9d8","permalink":"https://kenneth-lee-ch.github.io/project/dicemachine/","publishdate":"2018-08-10T00:00:00Z","relpermalink":"/project/dicemachine/","section":"project","summary":"We estimated probabilities for tetrahedral and triangular dice using the sphere projection method and multivariate calculus. To test our results, we printed several dice of varying sizes. After a few thousand initial rolls, we realized that our calculations weren’t accurately describing the situation and that bias could have been introduced due to rolling the dice by hand. Therefore, we built a machine to roll the dice.","tags":["Machine Learning","Computer Vision","Object Detection","Probability","Neural Networks","Robotics"],"title":"Meet Don, the Autonomous Dice Rolling Machine","type":"project"},{"authors":["Kenneth Lee","Kathy Pulotu"],"categories":null,"content":"","date":1530403200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530403200,"objectID":"34d46378fbadd3e54950b0d92ae75349","permalink":"https://kenneth-lee-ch.github.io/publication/poster/gsssecond/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/publication/poster/gsssecond/","section":"publication","summary":"Homegrown exit surveys tend to expand over time to include everything but the kitchen sink.  This session outlines how a small Institutional Research (IR) office utilized a student research team to condense and improve the Graduating Student Survey (GSS) at BYU-Hawaii.","tags":["Factor Analysis","Survey Design","Institutional Research","Data Visualization","Tableau"],"title":"Graduating Student Survey Revision: A student effort","type":"publication"},{"authors":["Kathy Pulotu","Kenneth Lee","Trysta Vallabh","Hong Ni Mui","Rosalind Ram"],"categories":null,"content":"","date":1530403200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530403200,"objectID":"a68492751851b4913429f6e4210cf78e","permalink":"https://kenneth-lee-ch.github.io/publication/poster/gssfirst/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/poster/gssfirst/","section":"publication","summary":"This poster outlines steps our student research team at BYU-Hawaii took to answer this question. Results from factor analysis, a review of literature and existing similar surveys, and information gleaned from faculty and staff members informed the survey revision pilot. Results of the pilot and next steps will be shared.","tags":["Factor Analysis","R","Institutional Research","Data Science"],"title":"How Meaningful is our Graduating Student Survey?","type":"publication"},{"authors":null,"categories":null,"content":"We use R and Jupyter Notebook in making predictions models to understand patient influx and predict their arrival times. Using these predictions, we can aid Pali Momi hospital in understanding what factors cause over and understaffing. Therefore, we can more positively help Pali Momi hospital to meet patient demand.\nTo enhance the given data from the hospital and provide more accurate predictions, we add the following features and variables: 1. Number of walk-ins 2. Airplane passenger arrival counts from the past two years from international and domestic individuals 3. Month of year, week of year 4. Scheduling periods 5. Doctor-to-patient ratio.\nIn this work, we use repeated cross-validation with 10 folds and 3 repeats, a common standard configuration for comparing models by estimating the model accuracy. Repeated K-fold Cross-Validation method involves splitting the dataset into k-subsets. Each subset is held-out while the model is trained on all other subsets. This process is repeated until accuracy is determined for each instance in the dataset, and an overall accuracy estimate is provided.\n","date":1524787200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1524787200,"objectID":"faf39fd7dea2a065908cb213cb9ca82c","permalink":"https://kenneth-lee-ch.github.io/project/pali_momi_hospital/","publishdate":"2018-04-27T00:00:00Z","relpermalink":"/project/pali_momi_hospital/","section":"project","summary":"This project predicts the patient inflow at Pali Momi Hospital's Emergency Room (ER) and provides recommendations for how to optimize scheduling for their ER's doctors and mid-level providers.","tags":["Machine Learning","Regression","Data Science"],"title":"Predict Patient Inflow at Pali Momi Hospital’s Emergency Room","type":"project"},{"authors":["Vasu Chetty","Nathan Woodbury","Jacob Brewer","Kenneth Lee","Sean Warnick"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"1d6ac0af192b05c5041ca48bbc4294e9","permalink":"https://kenneth-lee-ch.github.io/publication/conference-paper/apply-nr-to-twitter/","publishdate":"2016-01-01T00:00:00Z","relpermalink":"/publication/conference-paper/apply-nr-to-twitter/","section":"publication","summary":"In this work we apply a systems-theoretic approach to identifying trend setters on Twitter.","tags":["Network Reconstruction","Social Network","System Theory","Causality"],"title":"Applying a Passive Network Reconstruction Technique to Twitter Data in Order to Identify Trend Setters","type":"publication"}]